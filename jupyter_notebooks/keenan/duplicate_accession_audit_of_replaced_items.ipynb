{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 50\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copied from pyencoded-tools/encodedcc.py to avoid dependency.\n",
    "class ENC_Key:\n",
    "    def __init__(self, keyfile, keyname):\n",
    "        if os.path.isfile(str(keyfile)):\n",
    "            keys_f = open(keyfile, 'r')\n",
    "            keys_json_string = keys_f.read()\n",
    "            keys_f.close()\n",
    "            keys = json.loads(keys_json_string)\n",
    "        else:\n",
    "            keys = keyfile\n",
    "        key_dict = keys[keyname]\n",
    "        self.authid = key_dict['key']\n",
    "        self.authpw = key_dict['secret']\n",
    "        self.server = key_dict['server']\n",
    "        if not self.server.endswith(\"/\"):\n",
    "            self.server += \"/\"\n",
    "\n",
    "            \n",
    "class ENC_Connection(object):\n",
    "    def __init__(self, key):\n",
    "        self.headers = {'content-type': 'application/json', 'accept': 'application/json'}\n",
    "        self.server = key.server\n",
    "        self.auth = (key.authid, key.authpw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define key if private data desired.\n",
    "key = ENC_Key(os.path.expanduser(\"~/keypairs.json\"), 'prod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get accessions for all replaced items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pull accession of all Items with replaced status.\n",
    "url = 'https://www.encodeproject.org/search/'\\\n",
    "      '?type=File&type=Dataset&type=Donor&type=Library'\\\n",
    "      '&type=Pipeline&type=Biosample&type=AntibodyLot&status=replaced'\\\n",
    "      '&limit=all&format=json'\n",
    "r = requests.get(url, auth=(key.authid, key.authpw))\n",
    "search_results = r.json()['@graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accessions = set()\n",
    "for result in search_results:\n",
    "    accessions.add(result['accession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for each accession and check length of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop.close()\n",
    "# loop = asyncio.new_event_loop()\n",
    "# asyncio.set_event_loop(loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Asyncio request.\n",
    "\n",
    "result_length = []\n",
    "bad_accessions = []\n",
    "request_auth = aiohttp.BasicAuth(key.authid, key.authpw)\n",
    "\n",
    "async def get_json(url, sem):\n",
    "    async with sem:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url, auth=request_auth) as resp:\n",
    "                return await resp.json()\n",
    "\n",
    "async def get_request(accession, sem):\n",
    "    url = 'https://www.encodeproject.org/'\\\n",
    "          'search/?type=Item&accession={}'\\\n",
    "          '&limit=all&format=json'.format(accession)\n",
    "    result = await get_json(url, sem)\n",
    "    search_results = result['@graph']\n",
    "    num_results = len(search_results)\n",
    "    result_length.append({'accession': accession,\n",
    "                          'result_length': num_results})\n",
    "    if num_results > 1:\n",
    "        bad_accessions.append({'accession': accession,\n",
    "                               'results': search_results})\n",
    "\n",
    "sem = asyncio.Semaphore(20)\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.gather(*[get_request(accession, sem) for accession in accessions]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search for each accession, count number of results.\n",
    "# counter = 0 \n",
    "# result_length = []\n",
    "# bad_accessions = []\n",
    "# for accession in accessions:\n",
    "#     url = 'https://www.encodeproject.org/search/'\\\n",
    "#           '?type=Item&accession={}'\\\n",
    "#           '&limit=all&format=json'.format(accession)\n",
    "#     r = requests.get(url, auth=(key.authid, key.authpw))\n",
    "#     search_results = r.json()['@graph']\n",
    "#     result_length.append({'accession': accession,\n",
    "#                           'result_length': len(search_results)})\n",
    "#     if len(search_results) > 1:\n",
    "#         bad_accessions.append({'accession': accession,\n",
    "#                                'results': search_results})\n",
    "#     counter += 1\n",
    "#     if counter % 100 == 0:\n",
    "#         print(\".\", end=\"\")\n",
    "#     if counter % 1000 == 0:\n",
    "#         print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure search results returned for each accession.\n",
    "#assert len(accessions) == counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result_length).result_length.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_accessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_accessions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "duplicate_accession_data = []\n",
    "for bad in bad_accessions:\n",
    "    for item in bad['results']:\n",
    "        duplicate_accession_data.append({'accession': item['accession'],\n",
    "                                         'file_format': item['file_format'],\n",
    "                                         'status': item['status'],\n",
    "                                         'dataset': item['dataset']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duplicate_accessions = pd.DataFrame(duplicate_accession_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicate_accessions.dataset.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associate duplicate accessions to Experiment lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiment_list = duplicate_accessions.dataset.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_ids = \"&@id=\".join(experiment_list)\n",
    "url = 'https://www.encodeproject.org/search/'\\\n",
    "      '?type=Item&limit=all&frame=embedded&@id={}'.format(search_ids)\n",
    "r = requests.get(url, auth=(key.authid, key.authpw))\n",
    "search_results = r.json()['@graph']\n",
    "search_id_map = {}\n",
    "for experiment in search_results:\n",
    "    search_id_map[experiment['@id']] = experiment['lab']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_accessions['lab'] = duplicate_accessions.dataset.apply(lambda x: search_id_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*sorted(duplicate_accessions.lab.unique()), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(duplicate_accessions.accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicate_accessions[duplicate_accessions.status == \"replaced\"].groupby(['lab',\n",
    "                                                                         'accession',\n",
    "                                                                         'status',\n",
    "                                                                         'file_format']).count().sort_index(0)[[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicate_accessions.groupby(['lab',\n",
    "                              'status',\n",
    "                              'dataset',\n",
    "                              'accession',\n",
    "                              'file_format']).count().sort_index(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicate_accessions.groupby(['accession',\n",
    "                              'status', 'file_format',\n",
    "                              'lab',\n",
    "                              'dataset',\n",
    "                              'file_format']).count().sort_index(1, 0).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duplicate_accessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for all replaced Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data of all replaced Items.\n",
    "replaced_data = []\n",
    "url = 'https://www.encodeproject.org/search/'\\\n",
    "      '?type=File&type=Dataset&type=Donor&type=Library'\\\n",
    "      '&type=Pipeline&type=Biosample&type=AntibodyLot&status=replaced'\\\n",
    "      '&frame=embedded&limit=all&format=json'\n",
    "r = requests.get(url, auth=(key.authid, key.authpw))\n",
    "search_results = r.json()['@graph']\n",
    "na = 'not_available'\n",
    "for result in search_results:\n",
    "    sub_by = result.get('submitted_by', {})\n",
    "    if isinstance(sub_by, str):\n",
    "        submitted_by = sub_by\n",
    "    else:\n",
    "        submitted_by = sub_by.get('title', na)\n",
    "    lab = result.get('lab', {})\n",
    "    if isinstance(lab, str):\n",
    "        lab_name = lab\n",
    "    else:\n",
    "        lab_name = lab.get('name', na)\n",
    "    item_data = {'accession': result['accession'],\n",
    "                 'submitted_by': submitted_by,\n",
    "                 'derived_from': result.get('derived_from', na),\n",
    "                 'superseded_by': result.get('superseded_by', na),\n",
    "                 'supersedes': result.get('supersedes', na),\n",
    "                 '@id': result['@id'],\n",
    "                 'alternate_accessions': result.get('alternate_accessions', na),\n",
    "                 'dataset': result.get('dataset', na),\n",
    "                 'lab_name': lab_name,\n",
    "                 'date_created': result.get('date_created', na),\n",
    "                 '@type': result['@type'][0],\n",
    "                 'output_type': result.get('output_type', na),\n",
    "                 'file_format': result.get('file_format', na),\n",
    "                 'assembly': result.get('assembly', na),\n",
    "                 'paired_with': result.get('paired_with', na),\n",
    "                 'paired_end': result.get('paired_end', na),\n",
    "                 'file_format_type': result.get('file_format_type', na),\n",
    "                 'technical_replicates': result.get('technical_replicates', na),\n",
    "                 'replicate_uuid': result.get('replicate', {}).get('uuid', na),\n",
    "                 'md5sum': result.get('md5sum', na),\n",
    "                 'content_md5sum': result.get('content_md5sum', na),\n",
    "                 'status': result['status'],\n",
    "                 'product_id': result.get('product_id', na),\n",
    "                 'culture_start_date': result.get('culture_start_date', na),\n",
    "                 'biosample_type': result.get('biosample_type', na),\n",
    "                 'description': result.get('description', na),\n",
    "                 'treatments': result.get('treatments', na)\n",
    "                }\n",
    "    replaced_data.append(item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replaced_data[900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(replaced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_lab_name(lab):\n",
    "    if isinstance(lab, str):\n",
    "        parse_lab = lab.replace(\"/\", \"\").replace(\"labs\", \"\")\n",
    "        return parse_lab\n",
    "    else:\n",
    "        return lab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd = pd.DataFrame(replaced_data)\n",
    "rd.lab_name = rd.lab_name.apply(lambda x: parse_lab_name(x))\n",
    "rd.loc[rd.assembly.apply(lambda x: len(x) == 0), 'assembly'] = 'empty_list'\n",
    "rd.loc[rd.superseded_by.apply(lambda x: len(x) == 0), 'superseded_by'] = 'empty_list'\n",
    "rd.loc[rd.supersedes.apply(lambda x: len(x) == 0), 'supersedes'] = 'empty_list'\n",
    "rd.loc[rd.derived_from.apply(lambda x: len(x) == 0), 'derived_from'] = 'empty_list'\n",
    "rd.loc[rd.technical_replicates.apply(lambda x: len(x) == 0), 'technical_replicates'] = 'empty_list'\n",
    "rd.loc[rd.alternate_accessions.apply(lambda x: len(x) == 0), 'alternate_accessions'] = 'empty_list'\n",
    "rd.loc[rd.treatments.apply(lambda x: len(x) == 0), 'treatments'] = 'empty_list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check to see if replacement is similar to replaced (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unique_fields(data):\n",
    "    drop_fields = ['@id',\n",
    "                   '@accession',\n",
    "                   'md5sum',\n",
    "                   'content_md5sum',\n",
    "                   'date_created']\n",
    "    data = {k: v for k, v in data.items() if k not in drop_fields}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replacement_data = []\n",
    "broken_pair = defaultdict(list)\n",
    "for accession in rd.accession.unique():\n",
    "    replaced_values = rd[rd.accession == accession].to_dict(orient='records')[0]\n",
    "    url = 'https://www.encodeproject.org/{}/?format=json'.format(accession)\n",
    "    r = requests.get(url, auth=(key.authid, key.authpw))\n",
    "    if (r.status_code == 200):\n",
    "        result = r.json()\n",
    "        sub_by = result.get('submitted_by', {})\n",
    "        if isinstance(sub_by, str):\n",
    "            submitted_by = sub_by\n",
    "        else:\n",
    "            submitted_by = sub_by.get('title', na)\n",
    "        lab = result.get('lab', {})\n",
    "        if isinstance(lab, str):\n",
    "            lab_name = lab\n",
    "        else:\n",
    "            lab_name = lab.get('name', na)\n",
    "        item_data = {'accession': result['accession'],\n",
    "                     'submitted_by': submitted_by,\n",
    "                     '@id': result['@id'],\n",
    "                     'alternate_accessions': result.get('alternate_accessions', na),\n",
    "                     'dataset': result.get('dataset', na),\n",
    "                     'lab_name': lab_name,\n",
    "                     'date_created': result.get('date_created', na),\n",
    "                     '@type': result['@type'][0],\n",
    "                     'output_type': result.get('output_type', na),\n",
    "                     'file_format': result.get('file_format', na),\n",
    "                     'assembly': result.get('assembly', na),\n",
    "                     'paired_with': result.get('paired_with', na),\n",
    "                     'paired_end': result.get('paired_end', na),\n",
    "                     'file_format_type': result.get('file_format_type', na),\n",
    "                     'technical_replicates': result.get('technical_replicates', na),\n",
    "                     'replicate_uuid': result.get('replicate', {}).get('uuid', na),\n",
    "                     'md5sum': result.get('md5sum', na),\n",
    "                     'content_md5sum': result.get('content_md5sum', na),\n",
    "                     'status': result['status'],\n",
    "                     'product_id': result.get('product_id', na),\n",
    "                     'culture_start_date': result.get('culture_start_date', na),\n",
    "                     'biosample_type': result.get('biosample_type', na),\n",
    "                     'description': result.get('description', na),\n",
    "                     'treatments': result.get('treatments', na)\n",
    "                    }\n",
    "        item_temp = pd.DataFrame([item_data])\n",
    "        item_temp.lab_name = item_temp.lab_name.apply(lambda x: parse_lab_name(x))\n",
    "        item_temp.loc[item_temp.assembly.apply(lambda x: len(x) == 0), 'assembly'] = 'empty_list'\n",
    "        item_temp.loc[item_temp.technical_replicates.apply(lambda x: len(x) == 0), 'technical_replicates'] = 'empty_list'\n",
    "        item_temp.loc[item_temp.alternate_accessions.apply(lambda x: len(x) == 0), 'alternate_accessions'] = 'empty_list'\n",
    "        item_temp.loc[item_temp.treatments.apply(lambda x: len(x) == 0), 'treatments'] = 'empty_list'\n",
    "        item_temp = item_temp.to_dict(orient='records')[0]\n",
    "        replaced_dict = drop_unique_fields(replaced_values)\n",
    "        replacement_dict = drop_unique_fields(replaced_dict)\n",
    "        if replaced_dict != replacement_dict:\n",
    "            broken_pair['accession'].append(item_data)\n",
    "        replacement_data.append(item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(replacement_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for portal redirect of replaced accessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loop.close()\n",
    "# loop = asyncio.new_event_loop()\n",
    "# asyncio.set_event_loop(loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Asyncio request.\n",
    "replaced_by_file = []\n",
    "na = 'not_available'\n",
    "\n",
    "async def get_request(session, accession):\n",
    "        url = 'https://www.encodeproject.org/{}'.format(accession)\n",
    "        async with session.get(url, auth=request_auth, timeout=None) as response:\n",
    "                if response.status == 404:\n",
    "                    item_data = {'searched_accession': accession,\n",
    "                                 'redirected_to_accession': 'no_result'}\n",
    "                    replaced_by_file.append(item_data)\n",
    "                else:\n",
    "                    result = await response.json()\n",
    "                    sub_by = result.get('submitted_by', {})\n",
    "                    if isinstance(sub_by, str):\n",
    "                        submitted_by = sub_by\n",
    "                    else:\n",
    "                        submitted_by = sub_by.get('title', na)\n",
    "                    lab = result.get('lab', {})\n",
    "                    if isinstance(lab, str):\n",
    "                        lab_name = lab\n",
    "                    else:\n",
    "                        lab_name = lab.get('name', na)\n",
    "                    item_data = {'accession': result['accession'],\n",
    "                                 'submitted_by': submitted_by,\n",
    "                                 'derived_from': result.get('derived_from', na),\n",
    "                                 'superseded_by': result.get('superseded_by', na),\n",
    "                                 'supersedes': result.get('supersedes', na),\n",
    "                                 '@id': result['@id'],\n",
    "                                 'alternate_accessions': result.get('alternate_accessions', na),\n",
    "                                 'dataset': result.get('dataset', na),\n",
    "                                 'lab_name': lab_name,\n",
    "                                 'date_created': result.get('date_created', na),\n",
    "                                 '@type': result['@type'][0],\n",
    "                                 'output_type': result.get('output_type', na),\n",
    "                                 'file_format': result.get('file_format', na),\n",
    "                                 'assembly': result.get('assembly', na),\n",
    "                                 'paired_with': result.get('paired_with', na),\n",
    "                                 'paired_end': result.get('paired_end', na),\n",
    "                                 'file_format_type': result.get('file_format_type', na),\n",
    "                                 'technical_replicates': result.get('technical_replicates', na),\n",
    "                                 'replicate_uuid': result.get('replicate', {}).get('uuid', na),\n",
    "                                 'md5sum': result.get('md5sum', na),\n",
    "                                 'content_md5sum': result.get('content_md5sum', na),\n",
    "                                 'status': result['status'],\n",
    "                                 'product_id': result.get('product_id', na),\n",
    "                                 'culture_start_date': result.get('culture_start_date', na),\n",
    "                                 'biosample_type': result.get('biosample_type', na),\n",
    "                                 'description': result.get('description', na),\n",
    "                                 'treatments': result.get('treatments', na)}\n",
    "                    replaced_by_file.append(item_data)\n",
    "                if len(replaced_by_file) % 100 == 0:\n",
    "                    print(len(replaced_by_file))\n",
    "    \n",
    "async def create_session(accessions, loop):\n",
    "    connector = aiohttp.TCPConnector(keepalive_timeout=10, limit=100)\n",
    "    async with aiohttp.ClientSession(connector=connector, loop=loop) as session:\n",
    "        results = await asyncio.gather(*[get_request(session, accession) for accession in accessions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(create_session(accessions, loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(replaced_by_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Asyncio request.\n",
    "\n",
    "# request_auth = aiohttp.BasicAuth(key.authid, key.authpw)\n",
    "\n",
    "# replaced_by_file = []\n",
    "# na = 'not_available'\n",
    "\n",
    "# async def get_request(url, sem):\n",
    "#     async with sem:\n",
    "#         async with aiohttp.ClientSession() as session:\n",
    "#             async with session.get(url, auth=request_auth) as resp:\n",
    "#                 return await resp.json()\n",
    "\n",
    "# async def get_data(accession, sem):\n",
    "#     url = 'https://www.encodeproject.org/{}'.format(accession)\n",
    "#     result = await get_request(url, sem)\n",
    "#     if result.get('code', False) == 404:\n",
    "#         item_data = {'searched_accession': accession,\n",
    "#                      'redirected_to_accession': 'no_result'}\n",
    "#         replaced_by_file.append(item_data)\n",
    "#     else:\n",
    "#         sub_by = result.get('submitted_by', {})\n",
    "#         if isinstance(sub_by, str):\n",
    "#             submitted_by = sub_by\n",
    "#         else:\n",
    "#             submitted_by = sub_by.get('title', na)\n",
    "#         lab = result.get('lab', {})\n",
    "#         if isinstance(lab, str):\n",
    "#             lab_name = lab\n",
    "#         else:\n",
    "#             lab_name = lab.get('name', na)\n",
    "#         item_data = {'accession': result['accession'],\n",
    "#                      'submitted_by': submitted_by,\n",
    "#                      'derived_from': result.get('derived_from', na),\n",
    "#                      'superseded_by': result.get('superseded_by', na),\n",
    "#                      'supersedes': result.get('supersedes', na),\n",
    "#                      '@id': result['@id'],\n",
    "#                      'alternate_accessions': result.get('alternate_accessions', na),\n",
    "#                      'dataset': result.get('dataset', na),\n",
    "#                      'lab_name': lab_name,\n",
    "#                      'date_created': result.get('date_created', na),\n",
    "#                      '@type': result['@type'][0],\n",
    "#                      'output_type': result.get('output_type', na),\n",
    "#                      'file_format': result.get('file_format', na),\n",
    "#                      'assembly': result.get('assembly', na),\n",
    "#                      'paired_with': result.get('paired_with', na),\n",
    "#                      'paired_end': result.get('paired_end', na),\n",
    "#                      'file_format_type': result.get('file_format_type', na),\n",
    "#                      'technical_replicates': result.get('technical_replicates', na),\n",
    "#                      'replicate_uuid': result.get('replicate', {}).get('uuid', na),\n",
    "#                      'md5sum': result.get('md5sum', na),\n",
    "#                      'content_md5sum': result.get('content_md5sum', na),\n",
    "#                      'status': result['status'],\n",
    "#                      'product_id': result.get('product_id', na),\n",
    "#                      'culture_start_date': result.get('culture_start_date', na),\n",
    "#                      'biosample_type': result.get('biosample_type', na),\n",
    "#                      'description': result.get('description', na),\n",
    "#                      'treatments': result.get('treatments', na)\n",
    "#                     }\n",
    "#         replaced_by_file.append(item_data)\n",
    "        \n",
    "# sem = asyncio.Semaphore(100)\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(asyncio.gather(*[get_data(accession, sem) for accession in accessions]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(create_session(accessions, loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # For every replaced accession:\n",
    "# # Check if https://www.encodeproject.org/{accession} returns anything.\n",
    "# # If so, does it match replaced file type?\n",
    "# replaced_by_file = []\n",
    "# na = 'not_available'\n",
    "# for accession in accessions:\n",
    "#     url = 'https://www.encodeproject.org/{}'.format(accession)\n",
    "#     r = requests.get(url, auth=(key.authid, key.authpw))\n",
    "#     if r.status_code == 404:\n",
    "#         item_data = {'searched_accession': accession,\n",
    "#                      'redirected_to_accession': 'no_result'}\n",
    "#         replaced_by_file.append(item_data)\n",
    "#     else:\n",
    "#         result = r.json()\n",
    "#         sub_by = result.get('submitted_by', {})\n",
    "#         if isinstance(sub_by, str):\n",
    "#             submitted_by = sub_by\n",
    "#         else:\n",
    "#             submitted_by = sub_by.get('title', na)\n",
    "#         lab = result.get('lab', {})\n",
    "#         if isinstance(lab, str):\n",
    "#             lab_name = lab\n",
    "#         else:\n",
    "#             lab_name = lab.get('name', na)\n",
    "#         item_data = {'accession': result['accession'],\n",
    "#                      'submitted_by': submitted_by,\n",
    "#                      'derived_from': result.get('derived_from', na),\n",
    "#                      'superseded_by': result.get('superseded_by', na),\n",
    "#                      'supersedes': result.get('supersedes', na),\n",
    "#                      '@id': result['@id'],\n",
    "#                      'alternate_accessions': result.get('alternate_accessions', na),\n",
    "#                      'dataset': result.get('dataset', na),\n",
    "#                      'lab_name': lab_name,\n",
    "#                      'date_created': result.get('date_created', na),\n",
    "#                      '@type': result['@type'][0],\n",
    "#                      'output_type': result.get('output_type', na),\n",
    "#                      'file_format': result.get('file_format', na),\n",
    "#                      'assembly': result.get('assembly', na),\n",
    "#                      'paired_with': result.get('paired_with', na),\n",
    "#                      'paired_end': result.get('paired_end', na),\n",
    "#                      'file_format_type': result.get('file_format_type', na),\n",
    "#                      'technical_replicates': result.get('technical_replicates', na),\n",
    "#                      'replicate_uuid': result.get('replicate', {}).get('uuid', na),\n",
    "#                      'md5sum': result.get('md5sum', na),\n",
    "#                      'content_md5sum': result.get('content_md5sum', na),\n",
    "#                      'status': result['status'],\n",
    "#                      'product_id': result.get('product_id', na),\n",
    "#                      'culture_start_date': result.get('culture_start_date', na),\n",
    "#                      'biosample_type': result.get('biosample_type', na),\n",
    "#                      'description': result.get('description', na),\n",
    "#                      'treatments': result.get('treatments', na)\n",
    "#                     }\n",
    "#         replaced_by_file.append(item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(replaced_by_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = pd.DataFrame(replaced_by_file)\n",
    "rbf = rbf.fillna('is_null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rbf.lab_name = rbf.lab_name.apply(lambda x: parse_lab_name(x))\n",
    "rbf.loc[rbf.assembly.apply(lambda x: len(x) == 0), 'assembly'] = 'empty_list'\n",
    "rbf.loc[rbf.technical_replicates.apply(lambda x: len(x) == 0), 'technical_replicates'] = 'empty_list'\n",
    "rbf.loc[rbf.superseded_by.apply(lambda x: len(x) == 0), 'superseded_by'] = 'empty_list'\n",
    "rbf.loc[rbf.supersedes.apply(lambda x: len(x) == 0), 'supersedes'] = 'empty_list'\n",
    "rbf.loc[rbf.derived_from.apply(lambda x: len(x) == 0), 'derived_from'] = 'empty_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('replaced_items_no_redirect_06_12_2017.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['@type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df[df['@type'] == 'File']\n",
    "dff.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_assay_type(experiment):\n",
    "    url = 'https://www.encodeproject.org{}?format=json'.format(experiment)\n",
    "    r = requests.get(url, auth=(key.authid, key.authpw))\n",
    "    result = r.json()\n",
    "    return result.get('assay_term_name', 'na')\n",
    "def get_lab_name(experiment):\n",
    "    url = 'https://www.encodeproject.org/{}/?format=json'.format(experiment)\n",
    "    r = requests.get(url, auth=(key.authid, key.authpw))\n",
    "    result = r.json()\n",
    "    return result.get('lab', {}).get('name', 'na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['assay_type'] = dff.dataset.apply(lambda x: get_assay_type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dff.assay_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['experiment_lab'] = dff.dataset.apply(lambda x: get_lab_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rbf.to_csv(\"replaced_by_search.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge redirect data with replaced Item data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_redirect_accessions = rd[rd.accession.isin(rbf[rbf.redirected_to_accession == \"no_result\"].searched_accession.values)]\n",
    "no_redirect_accessions = no_redirect_accessions.sort_values('@type').reset_index(drop=True)\n",
    "no_redirect_accessions.loc[no_redirect_accessions.description.apply(lambda x: len(x) == 0), 'description'] = 'empty_string'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_redirect_accessions['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_redirect_accessions.content_md5sum.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_redirect_accessions.description.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_redirect_accessions.lab_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_redirect_accessions['@type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_redirect_accessions[no_redirect_accessions.md5sum != \"not_available\"].accession.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_redirect_accessions[no_redirect_accessions.md5sum != \"not_available\"].accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(no_redirect_accessions[no_redirect_accessions.md5sum == 'not_available'].accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.to_excel('replaced_items_no_redirect_06_12_2017.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for possible replacement files with same MD5sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# possible_replacements = defaultdict(list)\n",
    "# for md5 in no_redirect_accessions.md5sum.unique()[1:]:\n",
    "#     url = 'https://www.encodeproject.org/search/'\\\n",
    "#           '?type=Item&md5sum={}&status%21=replaced'\\\n",
    "#           '&frame=embedded&limit=all&format=json'.format(md5)\n",
    "#     r = requests.get(url, auth=(key.authid, key.authpw))\n",
    "#     if (r.status_code == 404) or (len(r.json()['@graph']) == 0):\n",
    "#         item_data = {'md5sum': md5,\n",
    "#                      'accession': 'no_result'}\n",
    "#         possible_replacements[md5].append(item_data)\n",
    "#     else:\n",
    "#         results = r.json()['@graph']\n",
    "#         for result in results:\n",
    "#             lab = result.get('lab', {})\n",
    "#             if isinstance(lab, str):\n",
    "#                 lab_name = lab\n",
    "#             else:\n",
    "#                 lab_name = lab.get('name', na)\n",
    "#             possible_replacements[md5].append({'accession': result['accession'],\n",
    "#                                                '@id': result['@id'],\n",
    "#                                                'alternate_accessions': result.get('alternate_accessions', na),\n",
    "#                                                'dataset': result.get('dataset', na),\n",
    "#                                                'lab_name': lab_name,\n",
    "#                                                'date_created': result.get('date_created', na),\n",
    "#                                                '@type': result['@type'][0],\n",
    "#                                                'output_type': result.get('output_type', na),\n",
    "#                                                'file_format': result.get('file_format', na),\n",
    "#                                                'assembly': result.get('assembly', na),\n",
    "#                                                'paired_with': result.get('paired_with', na),\n",
    "#                                                'paired_end': result.get('paired_end', na),\n",
    "#                                                'file_format_type': result.get('file_format_type', na),\n",
    "#                                                'technical_replicates': result.get('technical_replicates', na),\n",
    "#                                                'replicate_uuid': result.get('replicate', {}).get('uuid', na),\n",
    "#                                                'md5sum': result.get('md5sum', na),\n",
    "#                                                'content_md5sum': result.get('content_md5sum', na),\n",
    "#                                                'status': result['status']\n",
    "#                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loop.close()\n",
    "loop = asyncio.new_event_loop()\n",
    "asyncio.set_event_loop(loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_replacements = defaultdict(list)\n",
    "\n",
    "async def get_request(session, md5):\n",
    "    url = 'https://www.encodeproject.org/search/'\\\n",
    "                  '?type=Item&md5sum={}&status%21=replaced'\\\n",
    "                  '&frame=embedded&limit=all&format=json'.format(md5)\n",
    "    async with session.get(url, auth=request_auth) as response:\n",
    "        r = await response.json()\n",
    "        results = r['@graph']\n",
    "        if len(results) == 0:\n",
    "            item_data = {'md5sum': md5,\n",
    "                         'accession': 'no_result'}\n",
    "            possible_replacements[md5].append(item_data)\n",
    "        else:\n",
    "            for result in results:\n",
    "                lab = result.get('lab', {})\n",
    "                if isinstance(lab, str):\n",
    "                    lab_name = lab\n",
    "                else:\n",
    "                    lab_name = lab.get('name', na)\n",
    "                possible_replacements[md5].append({'accession': result['accession'],\n",
    "                                                   '@id': result['@id'],\n",
    "                                                   'alternate_accessions': result.get('alternate_accessions', na),\n",
    "                                                   'dataset': result.get('dataset', na),\n",
    "                                                   'lab_name': lab_name,\n",
    "                                                   'date_created': result.get('date_created', na),\n",
    "                                                   '@type': result['@type'][0],\n",
    "                                                   'output_type': result.get('output_type', na),\n",
    "                                                   'file_format': result.get('file_format', na),\n",
    "                                                   'assembly': result.get('assembly', na),\n",
    "                                                   'paired_with': result.get('paired_with', na),\n",
    "                                                   'paired_end': result.get('paired_end', na),\n",
    "                                                   'file_format_type': result.get('file_format_type', na),\n",
    "                                                   'technical_replicates': result.get('technical_replicates', na),\n",
    "                                                   'replicate_uuid': result.get('replicate', {}).get('uuid', na),\n",
    "                                                   'md5sum': result.get('md5sum', na),\n",
    "                                                   'content_md5sum': result.get('content_md5sum', na),\n",
    "                                                   'status': result['status']\n",
    "                                                  })\n",
    "\n",
    "async def create_session(md5s, loop):\n",
    "    conn = aiohttp.TCPConnector(keepalive_timeout=10, limit=100)\n",
    "    async with aiohttp.ClientSession(connector=conn, loop=loop) as session:\n",
    "        results = await asyncio.gather(*[get_request(session, md5) for md5 in md5s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(create_session(no_redirect_accessions.md5sum.unique()[1:], loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(possible_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possible_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_merge = [item for key, value in possible_replacements.items()\n",
    "                       for item in value if item['accession'] != 'no_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_merge = pd.DataFrame(possible_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_merge = possible_merge.rename(columns={'accession': 'possible_redirect_accession',\n",
    "                                                'status': 'possible_redirect_status'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_merge.loc[possible_merge.technical_replicates.apply(lambda x: len(x) == 0), 'technical_replicates'] = 'empty_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possible_merge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_matches = no_redirect_accessions[~(no_redirect_accessions.md5sum.isin(possible_merge.md5sum.values))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pm = possible_merge.merge(no_redirect_accessions,\n",
    "                          how='left',\n",
    "                          suffixes=('_new', '_old'),\n",
    "                          on=['md5sum',\n",
    "                              '@type',\n",
    "                              'file_format',\n",
    "                              'file_format_type'])[['md5sum',\n",
    "                                                    'accession',\n",
    "                                                    'status',\n",
    "                                                    'possible_redirect_accession',\n",
    "                                                    'possible_redirect_status',\n",
    "                                                    '@type',\n",
    "                                                    'file_format',\n",
    "                                                    'file_format_type',\n",
    "                                                    'assembly_old',\n",
    "                                                    'assembly_new',\n",
    "                                                    'dataset_old',\n",
    "                                                    'dataset_new',\n",
    "                                                    'date_created_old',\n",
    "                                                    'date_created_new',\n",
    "                                                    'lab_name_old',\n",
    "                                                    'lab_name_new',\n",
    "                                                    'technical_replicates_old',\n",
    "                                                    'technical_replicates_new',\n",
    "                                                    '@id_old',\n",
    "                                                    '@id_new',\n",
    "                                                    'output_type_old',\n",
    "                                                    'output_type_new',\n",
    "                                                    'paired_end_old',\n",
    "                                                    'paired_end_new',\n",
    "                                                    'paired_with_old',\n",
    "                                                    'paired_with_new',\n",
    "                                                    'replicate_uuid_old',\n",
    "                                                    'replicate_uuid_new',\n",
    "                                                    'alternate_accessions_old',\n",
    "                                                    'alternate_accessions_new',\n",
    "                                                    'content_md5sum_old',\n",
    "                                                    'content_md5sum_new']]\n",
    "pm#.to_excel('possible_redirect_accessions_for_replaced_files_06_12_2017.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_redirect_accessions[no_redirect_accessions.accession == 'ENCFF133IYK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pm.accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements_exact_match = pm[(pm.dataset_old == pm.dataset_new)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements_exact_match.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replacements_exact_match[[col for col in replacements_exact_match]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replacements_different = pm[~(pm.dataset_old == pm.dataset_new)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replacements_different.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replacements_different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different datasets but same MD5. Have to update replaced file to have replacement dataset.\n",
    "replacements_update_dataset = replacements_different[['@id_old', 'dataset_new']].rename(columns={'@id_old': '@id', 'dataset_new': 'dataset'})\n",
    "#replacements_update_dataset.to_csv('../../update_dataset_of_replaced_filed_matching_md5_06_27_2017.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now set exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replacements_patch = replacements_exact_match[['possible_redirect_accession',\n",
    "                                               'accession']].rename(columns={'accession': 'alternate_accessions:array',\n",
    "                                                                             'possible_redirect_accession': 'accession'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements_patch = replacements_patch.sort_values(\"alternate_accessions:array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements_patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list_patch = []\n",
    "for accession in replacements_patch.accession.unique():\n",
    "    data = {'accession': accession,\n",
    "            'alternate_accessions:array': \", \".join(replacements_patch[replacements_patch.accession == accession]\\\n",
    "                                                                                   ['alternate_accessions:array'].values)}\n",
    "    flat_list_patch.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replacements_patch_flat_list = pd.DataFrame(flat_list_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacements_patch_flat_list.to_csv('../../replaced_with_matching_replacements_patch_06_27_2017.tsv', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacements_different.sort_values('possible_redirect_accession').to_excel('replaced_same_md5_mismatched_dataset_06_14_2017.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the MD5sums with no matching replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_matching_md5_replacements = [item['md5sum'] for key, value in possible_replacements.items()\n",
    "                                               for item in value if item['accession'] == 'no_result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(pd.DataFrame(list(set(no_matching_md5_replacements))).rename(columns={0: 'md5sum'}).merge(no_redirect_accessions,\n",
    "                                                                                         how='left',\n",
    "                                                                                         on='md5sum')['accession'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for similar types of Files for possible replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_redirect_file = no_redirect_accessions[no_redirect_accessions['@type'] == 'File'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_redirect_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "na = 'not_available'\n",
    "possible_replacements = defaultdict(list)\n",
    "async def get_request_two(session, url, r):\n",
    "     async with session.get(url, auth=request_auth) as response_two:\n",
    "        result_one = await response_two.json()\n",
    "        search_results = result_one['@graph']\n",
    "        if len(search_results) == 0:\n",
    "            possible_replacements[r['accession']].append({'accession': r['accession'],\n",
    "                                                          'possible_replacement_accession': 'no_result'})\n",
    "        for result in search_results:\n",
    "                lab = result.get('lab', {})\n",
    "                sub_by = result.get('submitted_by', {})\n",
    "                if isinstance(sub_by, str):\n",
    "                    submitted_by = sub_by\n",
    "                else:\n",
    "                    submitted_by = sub_by.get('title', na)\n",
    "                if isinstance(lab, str):\n",
    "                    lab_name = lab\n",
    "                else:\n",
    "                    lab_name = lab.get('name', na)\n",
    "                possible_replacements[r['accession']].append({'accession': r['accession'],\n",
    "                                                              'possible_replacement_accession': result['accession'],\n",
    "                                                              '@id': result['@id'],\n",
    "                                                              'alternate_accessions': result.get('alternate_accessions', na),\n",
    "                                                              'dataset': result.get('dataset', na),\n",
    "                                                              'lab_name': lab_name,\n",
    "                                                              'date_created': result.get('date_created', na),\n",
    "                                                              '@type': result['@type'][0],\n",
    "                                                              'output_type': result.get('output_type', na),\n",
    "                                                              'file_format': result.get('file_format', na),\n",
    "                                                              'assembly': result.get('assembly', na),\n",
    "                                                              'paired_with': result.get('paired_with', na),\n",
    "                                                              'paired_end': result.get('paired_end', na),\n",
    "                                                              'file_format_type': result.get('file_format_type', na),\n",
    "                                                              'technical_replicates': result.get('technical_replicates', na),\n",
    "                                                              'replicate_uuid': result.get('replicate', {}).get('uuid', na),\n",
    "                                                              'md5sum': result.get('md5sum', na),\n",
    "                                                              'content_md5sum': result.get('content_md5sum', na),\n",
    "                                                              'status': result['status'],\n",
    "                                                              'submitted_by': submitted_by,\n",
    "                                                              'derived_from': result.get('derived_from', na),\n",
    "                                                              'superseded_by': result.get('superseded_by', na),\n",
    "                                                              'supersedes': result.get('supersedes', na)\n",
    "                                                              })\n",
    "    \n",
    "async def get_request_one(session, file_id):\n",
    "    url = 'https://www.encodeproject.org/{}/?format=json'.format(file_id)\n",
    "    async with session.get(url, auth=request_auth) as response_one:\n",
    "        result_one = await response_one.json()\n",
    "        r = result_one\n",
    "        file_format = r['file_format']\n",
    "        output_type = r['output_type']\n",
    "        dataset = r['dataset']\n",
    "        assembly = r.get('assembly', '*')\n",
    "        try:\n",
    "            assay_term_name = r['quality_metrics'][0]['assay_term_name']\n",
    "            url = 'https://www.encodeproject.org/search/?type=File&file_format={}'\\\n",
    "                  '&output_type={}&quality_metrics.assay_term_name={}'\\\n",
    "                  '&dataset={}&assembly={}&format=json&frame=embedded'\\\n",
    "                  '&status!=replaced'.format(file_format,\n",
    "                                             output_type,\n",
    "                                             assay_term_name,\n",
    "                                             dataset,\n",
    "                                             assembly)\n",
    "        except IndexError:\n",
    "            url = 'https://www.encodeproject.org/search/?type=File&file_format={}'\\\n",
    "                  '&output_type={}&dataset={}&assembly={}&format=json&frame=embedded'\\\n",
    "                  '&status!=replaced'.format(file_format,\n",
    "                                             output_type,\n",
    "                                             dataset,\n",
    "                                             assembly)\n",
    "        if assembly == '*':\n",
    "            url = url.replace('&assembly=*', '&assembly!=*')\n",
    "        result_two = await get_request_two(session, url, r)\n",
    "            \n",
    "async def create_session(file_ids, loop):\n",
    "    conn = aiohttp.TCPConnector(keepalive_timeout=10, limit=100)\n",
    "    async with aiohttp.ClientSession(connector=conn, loop=loop) as session:  \n",
    "        results = await asyncio.gather(*[get_request_one(session, file_id) for file_id in file_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(create_session(no_redirect_file['@id'].unique(), loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(possible_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "possible_replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in empty_lists for list fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "replacement_search = pd.DataFrame([item for key, value in possible_replacements.items() for item in value])\n",
    "replacement_search = replacement_search.fillna('isnull')\n",
    "replacement_search.loc[replacement_search.alternate_accessions.apply(lambda x: len(x) == 0), 'alternate_accessions'] = 'empty_list'\n",
    "replacement_search.loc[replacement_search.technical_replicates.apply(lambda x: len(x) == 0), 'technical_replicates'] = 'empty_list'\n",
    "replacement_search.loc[replacement_search.superseded_by.apply(lambda x: len(x) == 0), 'superseded_by'] = 'empty_list'\n",
    "replacement_search.loc[replacement_search.supersedes.apply(lambda x: len(x) == 0), 'supersedes'] = 'empty_list'\n",
    "replacement_search.loc[replacement_search.derived_from.apply(lambda x: len(x) == 0), 'derived_from'] = 'empty_list'\n",
    "\n",
    "no_redirect_file.loc[no_redirect_accessions.alternate_accessions.apply(lambda x: len(x) == 0), 'alternate_accessions'] = 'empty_list'\n",
    "no_redirect_file.loc[no_redirect_accessions.technical_replicates.apply(lambda x: len(x) == 0), 'technical_replicates'] = 'empty_list'\n",
    "no_redirect_file.loc[no_redirect_file.superseded_by.apply(lambda x: len(x) == 0), 'superseded_by'] = 'empty_list'\n",
    "no_redirect_file.loc[no_redirect_file.supersedes.apply(lambda x: len(x) == 0), 'supersedes'] = 'empty_list'\n",
    "no_redirect_file.loc[no_redirect_file.derived_from.apply(lambda x: len(x) == 0), 'derived_from'] = 'empty_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsm = replacement_search.merge(no_redirect_file,\n",
    "                          how='left',\n",
    "                          suffixes=('_new', '_old'),\n",
    "                          on=['accession'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substitute replaced file_ids with replacement file_ids in derived_from fields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm = rsm[~(rsm.status_new.isin(['revoked', 'deleted']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract lookup table from data with just one result.\n",
    "# If derived_from File doesn't redirect then look up and see possible replacement.\n",
    "# Use that as fill in value of comparison.\n",
    "dfl = rsm[(rsm.possible_replacement_accession != 'no_result')\n",
    "                          & (rsm.technical_replicates_old == rsm.technical_replicates_new)].drop_duplicates('accession',\n",
    "                                                                                                            keep=False).reset_index(drop=True)\n",
    "dfl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm[(rsm.possible_replacement_accession != 'no_result')\n",
    "    & (rsm.technical_replicates_old == rsm.technical_replicates_new)].drop_duplicates('accession',\n",
    "                                                                                      keep=False).reset_index(drop=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create from previous iterations below.\n",
    "derived_from_lookup = pd.concat([dfl, matching_rep.drop_duplicates('accession', keep=False)], axis=0).drop_duplicates('accession').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(derived_from_lookup.accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(id):\n",
    "    url = 'https://www.encodeproject.org/{}/?format=json'.format(id)\n",
    "    return requests.get(url, auth=(key.authid, key.authpw))\n",
    "\n",
    "def parse_derived_from(x):\n",
    "    if len(x) == 0 or x == 'not_available':\n",
    "        return x\n",
    "    new_list = []\n",
    "    for y in x:\n",
    "        y_id = y.split('/')[2]\n",
    "        if y_id.startswith('ENC'):\n",
    "            new_list.append(y)\n",
    "            continue\n",
    "        else:\n",
    "            r = get_json(y)\n",
    "            try:\n",
    "                accession = r.json()['accession']\n",
    "                r = get_json(accession)\n",
    "                if r.status_code == 404:\n",
    "                    # Pull from local lookup table.\n",
    "                    try:\n",
    "                        accession_replacement = derived_from_lookup[derived_from_lookup.accession == accession]\\\n",
    "                                                                      .possible_replacement_accession.values[0]\n",
    "                        new_list.append('/files/{}/'.format(accession_replacement))\n",
    "                    # If no results returned from one-result table.\n",
    "                    except IndexError:\n",
    "                        new_list.append(y)\n",
    "                else:\n",
    "                    accession_replacement = r.json()['accession']\n",
    "                    new_list.append('/files/{}/'.format(accession_replacement))\n",
    "            except KeyError:\n",
    "                print(y)\n",
    "                print(x)\n",
    "                new_list.append(y)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsm_derived_from_old = rsm.derived_from_old.apply(lambda x: parse_derived_from(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm.derived_from_old = rsm_derived_from_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm[~(rsm['@id_old'].isin(['/files/d9e23f37-9b33-41b9-b9df-0700ca87bc75/',\n",
    "                         '/files/3efeced1-a3c5-4131-a721-7c5f743350a9/',\n",
    "                         '/files/9fe192e9-af81-46f5-a16f-4d6b5cda577c/'])) & (rsm.supersedes_new != 'not_available')][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse lists for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_dict = {'_,e,i,l,m,p,s,t,t,y': 'empty_list',\n",
    "             'i,l,l,n,s,u': 'isnull',\n",
    "             '_,a,a,a,b,e,i,l,l,n,o,t,v': 'not_available'}\n",
    "def parse_list(x):\n",
    "    return ','.join([y.strip() for y in sorted(x)])\n",
    "rsm.date_created_old = rsm.date_created_old.apply(lambda x: pd.to_datetime(x))\n",
    "for field in ['technical_replicates_old',\n",
    "              'technical_replicates_new',\n",
    "              'superseded_by_old',\n",
    "              'superseded_by_new',\n",
    "              'supersedes_old',\n",
    "              'supersedes_new',\n",
    "              'derived_from_old',\n",
    "              'derived_from_new']:\n",
    "    rsm[field] = rsm[field].apply(lambda x: parse_list(x)).apply(lambda x: lazy_dict[x] if x in lazy_dict.keys() else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm[rsm.technical_replicates_old != rsm.technical_replicates_new][['technical_replicates_old',\n",
    "                                                                   'technical_replicates_new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm[rsm.accession == 'ENCFF721IVN'][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm[rsm.derived_from_old != rsm.derived_from_new][['derived_from_old', 'derived_from_new']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching content_md5sum, ready to patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_patch = rsm[(rsm.content_md5sum_old == rsm.content_md5sum_new)\n",
    "    & (rsm.content_md5sum_old != 'not_available')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cols = ['accession', 'possible_replacement_accession']\n",
    "cols = first_cols + [col for col in sorted(rsm_patch.columns, reverse=True) if col not in first_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm_patch[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rsm_patch[['possible_replacement_accession', 'accession']].rename(columns={'possible_replacement_accession': 'accession', 'accession': 'alternate_accessions:list'}).to_csv('../../matching_content_md5sum_patch_06_29_2017.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove files to be patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm = rsm[~(rsm.accession.isin(rsm_patch.accession.values))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Files that need replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rsm.accession.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible replacement with zero results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_zero_result = rsm[rsm.possible_replacement_accession == 'no_result'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rsm_zero_result.accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_zero_result.submitted_by_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm_zero_result[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set to deleted because no conservative IDR anymore.\n",
    "#rsm_zero_result.loc[rsm_zero_result.submitted_by_old == 'J. Seth Strattan', 'status_old'] = 'deleted'\n",
    "#rsm_zero_result[rsm_zero_result.submitted_by_old == 'J. Seth Strattan'][['@id_old', 'status_old']].rename(columns={'status_old': 'status', '@id_old': '@id'}).to_csv('../../zero_match_replaced_to_deleted_patch_06_28_2017.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for superseded_by/supersedes field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_zero_result.superseded_by_old.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible replacement with one result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result = rsm[rsm.possible_replacement_accession != 'no_result'].drop_duplicates('accession',\n",
    "                                                                                        keep=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rsm_one_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result.submitted_by_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result = rsm_one_result[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsm_one_result[rsm_one_result.submitted_by_old == \"Diane Trout\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for superseded_by/supersedes field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result.superseded_by_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result.supersedes_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result.superseded_by_new.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rsm_one_result.supersedes_new.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files that should be revoked instead of replaced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm_one_result[(rsm_one_result.superseded_by_old != 'empty_list')][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result_patch = rsm_one_result[(rsm_one_result.superseded_by_old != 'empty_list')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result_patch[['accession', 'superseded_by_old']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove files with superseded_by values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result = rsm_one_result[~(rsm_one_result.accession.isin(rsm_one_result_patch.accession.values))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsm_one_result[rsm_one_result.derived_from_old != rsm_one_result.derived_from_new][cols].submitted_by_old.value_counts() #[['derived_from_old', 'derived_from_new']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsm_one_result[(rsm_one_result.derived_from_old != rsm_one_result.derived_from_new)\n",
    "              & (rsm_one_result.submitted_by_old == 'Anna Vlasova')][cols][['accession', 'possible_replacement_accession', 'derived_from_old', 'derived_from_new']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsm[(rsm['@type_old'] == 'File')]['@id_old'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacements with one result and matching derived_from files != not_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsm_one_result[(rsm_one_result.derived_from_old == rsm_one_result.derived_from_new)\n",
    "               & (rsm_one_result.derived_from_old != 'not_available')].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacements with one result and derived_from both equal to not_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result[(rsm_one_result.derived_from_old == rsm_one_result.derived_from_new)\n",
    "               & (rsm_one_result.derived_from_old == 'not_available')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch one of Diane's that has missing derived_from but otherwise equal\n",
    "# dp = rsm_one_result[(rsm_one_result.derived_from_old == rsm_one_result.derived_from_new)\n",
    "#                & (rsm_one_result.derived_from_old == 'not_available')]\n",
    "# dp[['possible_replacement_accession', 'accession']].rename(columns={'possible_replacement_accession': 'accession',\n",
    "#                                                                     'accession': 'alternate_accessions:list'}).to_csv('../../one_match_missing_derived_from_patch_06_28_2017.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch 58 narrowPeaks with one match after dropping revoked/deleted from possible replacements\n",
    "# rsm_one_result[['possible_replacement_accession',\n",
    "#                 'accession']].rename(columns={'possible_replacement_accession': 'accession',\n",
    "#                                               'accession': 'alternate_accessions:list'}).to_csv('../../one_match_after_dropping_deleted_revoked_patch_06_30_2017.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacements with one result where derived_from_old but not derived_from_new equal to not_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result[(rsm_one_result.derived_from_old != rsm_one_result.derived_from_new)\n",
    "               & (rsm_one_result.derived_from_old == 'not_available')].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacements with one result where derived_from_new but not derived_from_old equal to not_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result[(rsm_one_result.derived_from_old != rsm_one_result.derived_from_new)\n",
    "               & (rsm_one_result.derived_from_new == 'not_available')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacements with one result where either are not_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result[(rsm_one_result.derived_from_old == 'not_available')\n",
    "               | (rsm_one_result.derived_from_new == 'not_available')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacements with one result where derived_from not matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm_one_result[rsm_one_result.derived_from_old != rsm_one_result.derived_from_new].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacements with one result where derived_from is matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result[rsm_one_result.derived_from_old == rsm_one_result.derived_from_new].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result_full_match = rsm_one_result[(rsm_one_result.derived_from_old == rsm_one_result.derived_from_new)\n",
    "                                           & (rsm_one_result.derived_from_old != 'not_available')][cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result_full_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rsm_one_result_full_match.possible_replacement_accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm_one_result_full_match[['possible_replacement_accession', 'accession']].rename(columns={'possible_replacement_accession': 'accession',\n",
    "                                                                                           'accession': 'alternate_accessions:list'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacements with one result with no matching derived_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsm_one_result_no_match = rsm_one_result[~(rsm_one_result.accession.isin(rsm_one_result_full_match.accession.values))][cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result_no_match.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm_one_result_no_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rsm_one_result_no_match.file_format_type_new.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm_one_result_no_match[rsm_one_result_no_match.file_format_type_new == \"not_available\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm_one_result_no_match[['derived_from_new', 'derived_from_old']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_one_result_no_match[rsm_one_result_no_match.submitted_by_old == 'J. Seth Strattan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Patch these narrowPeaks that match except for derived_from because upstream Files changed.\n",
    "sp = rsm_one_result_no_match[rsm_one_result_no_match.submitted_by_old == 'J. Seth Strattan'][['possible_replacement_accession', 'accession']]\n",
    "sp.rename(columns={'possible_replacement_accession': 'accession',\n",
    "                   'accession': 'alternate_accessions:list'})#.to_csv('../../one_match_derived_from_mismatch_patch_06_28_2017.tsv', index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible replacement with many results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_multi_result = rsm[rsm.duplicated('accession', keep=False)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rsm_multi_result.accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_multi_result.drop_duplicates('accession', keep='first').reset_index().submitted_by_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsm_multi_result[rsm_multi_result.accession == 'ENCFF719FSK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups add back up to total number of accessions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(rsm_zero_result) + len(rsm_one_result) + len(rsm_one_result_patch) + len(rsm_multi_result.accession.unique()) == len(rsm.accession.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does matching on technical replicates and derived_from reduce number of possible replacements with many results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_rep = rsm_multi_result[(rsm_multi_result.technical_replicates_old == rsm_multi_result.technical_replicates_new)\n",
    "                                & (rsm_multi_result.derived_from_old == rsm_multi_result.derived_from_new)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matching_rep.accession.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiresults that now only have one result after matching on technical_replicate and derived_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matching_rep.drop_duplicates('accession', keep=False).accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rsm_multi_one_result = matching_rep.drop_duplicates('accession', keep=False)[cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rsm_multi_one_result[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rsm_multi_one_result[['possible_replacement_accession', 'accession']].rename(columns={'possible_replacement_accession': 'accession',\n",
    "#                                                                                       'accession': 'alternate_accessions:list'}).to_csv('../../multi_one_match_patch_06_27_2017.tsv',\n",
    "#                                                                                                                                        index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch multiresults that have one match when matched on tech_rep (only narrowPeaks)\n",
    "# multi_one_narrow_peaks = rsm_multi_result[(rsm_multi_result.technical_replicates_old == rsm_multi_result.technical_replicates_new)\n",
    "#                                          & (rsm_multi_result.file_format_type_old == 'narrowPeak')].drop_duplicates('accession', keep=False).reset_index(drop=True)\n",
    "# multi_one_narrow_peaks[['possible_replacement_accession', 'accession']].rename(columns={'possible_replacement_accession': 'accession',\n",
    "#                                                                            'accession': 'alternate_accessions:list'}).to_csv('../../multi_narrow_peaks_tech_rep_match_patch_06_30_2017.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiresults that still have more than one result after matching on technical_replicate and derived_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matching_rep[matching_rep.duplicated('accession', keep=False)].accession.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by accession and possible_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = ['accession','possible_replacement_accession']\n",
    "cols = cols + [x for x in sorted(rsm.columns, reverse=True) if (x not in cols) and (x not in ['alternate_accessions_new',\n",
    "                                                                                              'alternate_accessions_old'])]\n",
    "mr = matching_rep[matching_rep.duplicated('accession', keep=False)].groupby(cols).count().reset_index()\n",
    "matching_rep[matching_rep.duplicated('accession', keep=False)].groupby(cols).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Patch pointing to in progress replacement instead of deleted replacement.\n",
    "# in_prog_multi_patch = mr[(mr.status_new == 'in progress')\n",
    "#                          & (mr.accession.isin(['ENCFF219IZI',\n",
    "#                                                'ENCFF362CIL',\n",
    "#                                                'ENCFF522EVZ',\n",
    "#                                                'ENCFF526SQT',\n",
    "#                                                'ENCFF554QRY',\n",
    "#                                                'ENCFF799OIZ',\n",
    "#                                                'ENCFF826MUG',\n",
    "#                                                'ENCFF832XOD',\n",
    "#                                                'ENCFF833LEK']))]\n",
    "# # in_prog_multi_patch[['possible_replacement_accession', 'accession']].rename(columns={'possible_replacement_accession': 'accession',\n",
    "# #                                                                                      'accession': 'alternate_accessions:list'})\\\n",
    "# #                                         .to_csv('../../multi_result_point_to_in_progress_patch_06_28_2017.tsv', index=False, sep='\\t')\n",
    "# in_prog_multi_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Patch pointing to released replacement instead of revoked replacement.\n",
    "# released_multi_patch = mr[(mr.status_new == 'released')\n",
    "#                            & (mr.accession.isin(['ENCFF311CTD',\n",
    "#                                                  'ENCFF442FSP',\n",
    "#                                                  'ENCFF521DYG',\n",
    "#                                                  'ENCFF660PBO',\n",
    "#                                                  'ENCFF723DLE',\n",
    "#                                                  'ENCFF758WLI',\n",
    "#                                                  'ENCFF803YCX',\n",
    "#                                                  'ENCFF809POG']))]\n",
    "# # released_multi_patch[['possible_replacement_accession', 'accession']].rename(columns={'possible_replacement_accession': 'accession',\n",
    "# #                                                                                       'accession': 'alternate_accessions:list'})\\\n",
    "# #                                     .to_csv('../../multi_result_point_to_released_patch_06_28_2017.tsv', index=False, sep='\\t')\n",
    "# released_multi_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Patch these as deleted because merged fasta that was never released\n",
    "# mr.loc[mr.submitted_by_old == 'Xintao Wei', 'status_old'] = 'deleted'\n",
    "# mr[mr.submitted_by_old == 'Xintao Wei'].drop_duplicates('accession')[['@id_old', 'status_old']].rename(columns={'status_old': 'status', '@id_old': '@id'}).to_csv('../../two_match_to_deleted_patch_06_29_2017.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiresults that don't match on technical_replicates or derived_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_matching_rep = rsm_multi_result[~(rsm_multi_result.accession.isin(matching_rep.accession.unique()))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_matching_rep.accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_matching_rep[~(no_matching_rep.accession.isin(multi_tech_match.accession)) & (no_matching_rep.submitted_by_old == \"J. Seth Strattan\")]['@id_old'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiresults that have matching technical_replicates but not derived_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_matching_rep[(no_matching_rep.technical_replicates_old == no_matching_rep.technical_replicates_new)\n",
    "                & (no_matching_rep.derived_from_old != no_matching_rep.derived_from_new)].accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_matching_rep[(no_matching_rep.technical_replicates_old == no_matching_rep.technical_replicates_new)\n",
    "                & (no_matching_rep.derived_from_old != no_matching_rep.derived_from_new)].drop_duplicates('accession', keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_tech_match = no_matching_rep[(no_matching_rep.technical_replicates_old == no_matching_rep.technical_replicates_new)\n",
    "                & (no_matching_rep.derived_from_old != no_matching_rep.derived_from_new)]\n",
    "no_matching_rep[(no_matching_rep.technical_replicates_old == no_matching_rep.technical_replicates_new)\n",
    "                & (no_matching_rep.derived_from_old != no_matching_rep.derived_from_new)].groupby(cols).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tech_match.superseded_by_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_tech_match[multi_tech_match.superseded_by_old == 'empty_list'][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tech_match[multi_tech_match.supersedes_new != 'not_available'][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_tech_match.supersedes_old.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One result after matching on technical_replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tech_one_match = multi_tech_match.drop_duplicates('accession', keep=False)\n",
    "len(multi_tech_match.drop_duplicates('accession', keep=False).accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tech_one_match.submitted_by_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(multi_tech_one_match.output_type_old, multi_tech_one_match.submitted_by_old, margins=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tech_one_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete because no matching derived_from\n",
    "#multi_tech_one_match[['@id_old', 'status_old']].rename(columns={'@id_old': '@id', 'status_old': 'status'}).to_csv('../../no_matching_derived_from_delete_patch_07_03_2017.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multi_tech_one_match.file_format_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_tech_one_match[(multi_tech_one_match.output_type_old != 'alignments')][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_tech_one_match[(multi_tech_one_match.submitted_by_old == 'Xintao Wei')\n",
    "                    & (multi_tech_one_match.output_type_old != 'alignments')][cols]#[['@id_old', 'possible_replacement_accession']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_tech_one_match.groupby(cols).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tech_one_match.file_format_type_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_tech_one_match[multi_tech_one_match.submitted_by_old == \"Jean Davidson\"][cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiresult after matching on technical_replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multi_tech_match[multi_tech_match.duplicated('accession', keep=False)].accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtm = multi_tech_match[multi_tech_match.duplicated('accession', keep=False)]\n",
    "mtm.groupby(cols).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtm[mtm.submitted_by_old == 'Jean Davidson'].groupby(cols).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mtm[mtm.submitted_by_old == 'J. Seth Strattan'].groupby(cols).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiresults that have matching derived_from but not technical_replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_matching_rep[(no_matching_rep.technical_replicates_old != no_matching_rep.technical_replicates_new)\n",
    "                & (no_matching_rep.derived_from_old == no_matching_rep.derived_from_new)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_matching_rep[(no_matching_rep.technical_replicates_old != no_matching_rep.technical_replicates_new)\n",
    "                & (no_matching_rep.derived_from_old == no_matching_rep.derived_from_new)].groupby(cols).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiresults that have mismatching derived_from and technical_replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(no_matching_rep[(no_matching_rep.technical_replicates_old != no_matching_rep.technical_replicates_new)\n",
    "                & (no_matching_rep.derived_from_old != no_matching_rep.derived_from_new)].accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_matching_rep[(no_matching_rep.technical_replicates_old != no_matching_rep.technical_replicates_new)\n",
    "                & (no_matching_rep.derived_from_old != no_matching_rep.derived_from_new)].groupby(cols).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ['accession','possible_replacement_accession']\n",
    "cols = cols + [x for x in sorted(matching_rep.columns, reverse=True) if (x not in cols) and (x not in ['alternate_accessions_new',\n",
    "                                                                                                       'alternate_accessions_old'])]\n",
    "no_matching_rep.groupby(cols).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessions of multiple results that don't have matching technical_replicates or derived_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mis_matching_rep = rsm_multi_result[~(rsm_multi_result.accession.isin(matching_rep.accession))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mis_matching_rep.accession.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_matching_rep[['technical_replicates_old','technical_replicates_new', 'derived_from_old', 'derived_from_new']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull all accessions ready for patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replacement_patch = pd.concat([rsm_patch,\n",
    "                               rsm_one_result_full_match,\n",
    "                               rsm_multi_one_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squash list for patching.\n",
    "patch_list = []\n",
    "for replacement_accession in replacement_patch.possible_replacement_accession.unique():\n",
    "    values = replacement_patch[replacement_patch.possible_replacement_accession == replacement_accession]['accession']\n",
    "    accession_list = []\n",
    "    for val in values:\n",
    "        accession_list.append(val)\n",
    "    patch_list.append({'accession': replacement_accession,\n",
    "                       'alternate_accessions:array': ', '.join(accession_list)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_data = pd.DataFrame(patch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#patch_data.to_csv(\"replaced_with_matching_replacements_patch_06_21_2017.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.plotting_context(\"notebook\", font_scale=1.5):\n",
    "    fig = plt.figure(figsize=[14, 8])\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.stripplot(x='date_created_old',\n",
    "                  data=rsm[rsm.possible_replacement_accession == 'no_result'],\n",
    "                 size=10,\n",
    "                 color='black',\n",
    "                 alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biosamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biosamples = no_redirect_accessions[no_redirect_accessions['@type'] == 'Biosample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "biosamples.submitted_by.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = 'not_available'\n",
    "possible_replacements = defaultdict(list)\n",
    "async def get_request_two(session, url, r):\n",
    "     async with session.get(url, auth=request_auth) as response_two:\n",
    "        result_one = await response_two.json()\n",
    "        search_results = result_one['@graph']\n",
    "        if len(search_results) == 0:\n",
    "            possible_replacements[r['accession']].append({'accession': r['accession'],\n",
    "                                                          'possible_replacement_accession': 'no_result'})\n",
    "        for result in search_results:\n",
    "                lab = result.get('lab', {})\n",
    "                sub_by = result.get('submitted_by', {})\n",
    "                if isinstance(sub_by, str):\n",
    "                    submitted_by = sub_by\n",
    "                else:\n",
    "                    submitted_by = sub_by.get('title', na)\n",
    "                if isinstance(lab, str):\n",
    "                    lab_name = lab\n",
    "                else:\n",
    "                    lab_name = lab.get('name', na)\n",
    "                possible_replacements[r['accession']].append({'accession': r['accession'],\n",
    "                                                              'possible_replacement_accession': result['accession'],\n",
    "                                                              '@id': result['@id'],\n",
    "                                                              'alternate_accessions': result.get('alternate_accessions', na),\n",
    "                                                              'dataset': result.get('dataset', na),\n",
    "                                                              'lab_name': lab_name,\n",
    "                                                              'date_created': result.get('date_created', na),\n",
    "                                                              '@type': result['@type'][0],\n",
    "                                                              'output_type': result.get('output_type', na),\n",
    "                                                              'file_format': result.get('file_format', na),\n",
    "                                                              'assembly': result.get('assembly', na),\n",
    "                                                              'paired_with': result.get('paired_with', na),\n",
    "                                                              'paired_end': result.get('paired_end', na),\n",
    "                                                              'file_format_type': result.get('file_format_type', na),\n",
    "                                                              'technical_replicates': result.get('technical_replicates', na),\n",
    "                                                              'replicate_uuid': result.get('replicate', {}).get('uuid', na),\n",
    "                                                              'md5sum': result.get('md5sum', na),\n",
    "                                                              'content_md5sum': result.get('content_md5sum', na),\n",
    "                                                              'status': result['status'],\n",
    "                                                              'submitted_by': submitted_by,\n",
    "                                                              'derived_from': result.get('derived_from', na),\n",
    "                                                              'superseded_by': result.get('superseded_by', na),\n",
    "                                                              'supersedes': result.get('supersedes', na)\n",
    "                                                              })\n",
    "    \n",
    "async def get_request_one(session, file_id):\n",
    "    url = 'https://www.encodeproject.org/{}/?format=json'.format(file_id)\n",
    "    async with session.get(url, auth=request_auth) as response_one:\n",
    "        result_one = await response_one.json()\n",
    "        r = result_one\n",
    "        file_format = r['file_format']\n",
    "        output_type = r['output_type']\n",
    "        dataset = r['dataset']\n",
    "        assembly = r.get('assembly', '*')\n",
    "        try:\n",
    "            assay_term_name = r['quality_metrics'][0]['assay_term_name']\n",
    "            url = 'https://www.encodeproject.org/search/?type=File&file_format={}'\\\n",
    "                  '&output_type={}&quality_metrics.assay_term_name={}'\\\n",
    "                  '&dataset={}&assembly={}&format=json&frame=embedded'\\\n",
    "                  '&status!=replaced'.format(file_format,\n",
    "                                             output_type,\n",
    "                                             assay_term_name,\n",
    "                                             dataset,\n",
    "                                             assembly)\n",
    "        except IndexError:\n",
    "            url = 'https://www.encodeproject.org/search/?type=File&file_format={}'\\\n",
    "                  '&output_type={}&dataset={}&assembly={}&format=json&frame=embedded'\\\n",
    "                  '&status!=replaced'.format(file_format,\n",
    "                                             output_type,\n",
    "                                             dataset,\n",
    "                                             assembly)\n",
    "        if assembly == '*':\n",
    "            url = url.replace('&assembly=*', '&assembly!=*')\n",
    "        result_two = await get_request_two(session, url, r)\n",
    "            \n",
    "async def create_session(file_ids, loop):\n",
    "    conn = aiohttp.TCPConnector(keepalive_timeout=10, limit=100)\n",
    "    async with aiohttp.ClientSession(connector=conn, loop=loop) as session:  \n",
    "        results = await asyncio.gather(*[get_request_one(session, file_id) for file_id in file_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "na = 'not_available'\n",
    "possible_biosample_replacements = defaultdict(list)\n",
    "for biosample_id in biosamples['@id'].unique():\n",
    "    r = requests.get('https://www.encodeproject.org/{}/?format=json'.format(biosample_id),\n",
    "                     auth=(key.authid, key.authpw))\n",
    "    r = r.json()\n",
    "    lab_old = r.get('lab', {})\n",
    "    if isinstance(lab_old, str):\n",
    "        lab_name_old = lab_old\n",
    "    else:\n",
    "        lab_name_old = lab_old.get('name', na)\n",
    "    donor_old = r.get('donor', {})\n",
    "    if isinstance(donor_old, str):\n",
    "        donor_name_old = donor_old\n",
    "    else:\n",
    "        donor_name_old = donor_old.get('@id', na)\n",
    "    sub_by_old = r.get('submitted_by', {})\n",
    "    if isinstance(sub_by_old, str):\n",
    "        submitted_by_old = sub_by_old\n",
    "    else:\n",
    "        submitted_by_old = sub_by_old.get('title', na)\n",
    "    try:\n",
    "        product_id = r['product_id']\n",
    "        health_status = r['health_status']\n",
    "        culture_start_date = r['culture_start_date']\n",
    "        url = 'https://www.encodeproject.org/search/'\\\n",
    "              '?type=Biosample&product_id={}'\\\n",
    "              '&health_status={}&culture_start_date={}'\\\n",
    "              '&status%21=replaced&format=json&frame=embedded'.format(product_id,\n",
    "                                                                      health_status,\n",
    "                                                                      culture_start_date)\n",
    "    except KeyError:\n",
    "         description = r['description']\n",
    "         url = 'https://www.encodeproject.org/search/'\\\n",
    "              '?type=Biosample&description={}'\\\n",
    "              '&status%21=replaced&format=json&frame=embedded'.format(description)\n",
    "    search_results = requests.get(url, auth=(key.authid, key.authpw))\n",
    "    search_results = search_results.json()['@graph']\n",
    "    if len(search_results) == 0:\n",
    "        possible_biosample_replacements[r['accession']].append({'accession': r['accession'],\n",
    "                                                                'possible_replacement_accession': 'no_result'})\n",
    "    for result in search_results:\n",
    "            lab_new = result.get('lab', {})\n",
    "            if isinstance(lab_new, str):\n",
    "                lab_name_new = lab_new\n",
    "            else:\n",
    "                lab_name_new = lab_new.get('name', na)\n",
    "            donor_new = result.get('donor', {})\n",
    "            if isinstance(donor_new, str):\n",
    "                donor_name_new = donor_new\n",
    "            else:\n",
    "                donor_name_new = donor_new.get('@id', na)\n",
    "            sub_by_new = result.get('submitted_by', {})\n",
    "            if isinstance(sub_by_new, str):\n",
    "                submitted_by_new = sub_by_new\n",
    "            else:\n",
    "                submitted_by_new = sub_by_new.get('title', na)\n",
    "            possible_biosample_replacements[r['accession']].append({'accession': r['accession'],\n",
    "                                                                    'possible_replacement_accession': result['accession'],\n",
    "                                                                    '@id_old': r['@id'],\n",
    "                                                                    '@id_new': result['@id'],\n",
    "                                                                    'alternate_accessions_new': r.get('alternate_accessions', na),\n",
    "                                                                    'alternate_accessions_old': result.get('alternate_accessions', na),\n",
    "                                                                    'donor_old': donor_name_old,\n",
    "                                                                    'donor_new': donor_name_new,\n",
    "                                                                    'lab_name_old': lab_name_old,\n",
    "                                                                    'lab_name_new': lab_name_new,\n",
    "                                                                    'date_created_old': r.get('date_created', na),\n",
    "                                                                    'date_created_new': result.get('date_created', na),\n",
    "                                                                    '@type_old': r['@type'][0],\n",
    "                                                                    '@type_new': result['@type'][0],\n",
    "                                                                    'status_old': r['status'],\n",
    "                                                                    'status_new': result['status'],\n",
    "                                                                    'product_id_old': r.get('product_id', na),\n",
    "                                                                    'product_id_new': result.get('product_id', na),\n",
    "                                                                    'health_status_old': r.get('health_status', na),\n",
    "                                                                    'health_status_new': result.get('health_status', na),\n",
    "                                                                    'culture_start_date_old': r.get('culture_start_date', na),\n",
    "                                                                    'culture_start_date_new': result.get('culture_start_date', na),\n",
    "                                                                    'biosample_type_old': r['biosample_type'],\n",
    "                                                                    'biosample_type_new': result['biosample_type'],\n",
    "                                                                    'treatment_old': r['treatments'],\n",
    "                                                                    'treatment_new': result['treatments'],\n",
    "                                                                    'biosample_term_name_old': r['biosample_term_name'],\n",
    "                                                                    'biosample_term_name_new': result['biosample_term_name'],\n",
    "                                                                    'summary_old': r['summary'],\n",
    "                                                                    'summary_new': result['summary'],\n",
    "                                                                    'description_old': r['description'],\n",
    "                                                                    'description_new': result['description'],\n",
    "                                                                    'pooled_from_old': r.get('pooled_from', na),\n",
    "                                                                    'pooled_from_new': result.get('pooled_from', na),\n",
    "                                                                    'part_of_old': r.get('part_of', na),\n",
    "                                                                    'part_of_new': result.get('part_of', na),\n",
    "                                                                    'culture_harvest_date_old': r.get('culture_harvest_date', na),\n",
    "                                                                    'culture_harvest_date_new': result.get('culture_harvest_date', na),\n",
    "                                                                    'passage_number_old': r.get('passage_number', na),\n",
    "                                                                    'passage_number_new': result.get('passage_number', na),\n",
    "                                                                    'lot_id_old': r.get('lot_id', na),\n",
    "                                                                    'lot_id_new': result.get('lot_id', na),\n",
    "                                                                    'submitted_by_old': submitted_by_old,\n",
    "                                                                    'submitted_by_new': submitted_by_new\n",
    "                                                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(possible_biosample_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_biosample_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_search = pd.DataFrame([item for key, value in possible_biosample_replacements.items() for item in value])\n",
    "replacement_search = replacement_search.fillna('isnull')\n",
    "replacement_search.loc[replacement_search.alternate_accessions_old.apply(lambda x: len(x) == 0), 'alternate_accessions_old'] = 'empty_list'\n",
    "replacement_search.loc[replacement_search.alternate_accessions_new.apply(lambda x: len(x) == 0), 'alternate_accessions_new'] = 'empty_list'\n",
    "#replacement_search.loc[replacement_search.pooled_from_old.apply(lambda x: len(x) == 0), 'pooled_from_old'] = 'empty_list'\n",
    "#replacement_search.loc[replacement_search.pooled_from_new.apply(lambda x: len(x) == 0), 'pooled_from_new'] = 'empty_list'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replacement_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_dict = {'_,e,i,l,m,p,s,t,t,y': 'empty_list',\n",
    "             'i,l,l,n,s,u': 'isnull',\n",
    "             '_,a,a,a,b,e,i,l,l,n,o,t,v': 'not_available'}\n",
    "def parse_list(x):\n",
    "    return ','.join([y.strip() for y in sorted(x)])\n",
    "replacement_search.date_created_old = replacement_search.date_created_old.apply(lambda x: pd.to_datetime(x))\n",
    "replacement_search.date_created_new = replacement_search.date_created_new.apply(lambda x: pd.to_datetime(x))\n",
    "for field in ['treatment_new',\n",
    "              'treatment_old',\n",
    "              'alternate_accessions_old',\n",
    "              'alternate_accessions_new',\n",
    "              'pooled_from_new',\n",
    "              'pooled_from_old',\n",
    "              'part_of_new',\n",
    "              'part_of_old']:\n",
    "    replacement_search[field] = replacement_search[field].apply(lambda x: parse_list(x)).apply(lambda x: lazy_dict[x] if x in lazy_dict.keys() else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biosamples_one_match = replacement_search.drop_duplicates('accession', keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cols = ['accession', 'possible_replacement_accession']\n",
    "bcols = first_cols + [col for col in sorted(biosamples_one_match.columns, reverse=True) if col not in first_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biosamples[biosamples['@id'].isin(replacement_search['@id_old'])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "biosamples_one_match[bcols].lab_name_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biosamples_one_match[bcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_patch = []\n",
    "for replacement in bs_patch.possible_replacement_accession.unique():\n",
    "    data = {'accession': replacement,\n",
    "            'alternate_accessions:array': \", \".join(bs_patch[bs_patch.possible_replacement_accession == replacement].accession.values)}\n",
    "    flat_patch.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = pd.DataFrame(flat_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp.to_csv('../../biosample_one_match_patch_07_03_2017.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biosamples_multi_match[biosamples_multi_match.accession.isin(bs_multi_match.accession)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_multi_match = bs[(bs.donor_old == bs.donor_old)\n",
    "   & (bs.passage_number_old == bs.passage_number_new)\n",
    "   & (bs.lot_id_old == bs.lot_id_new)\n",
    "   & (bs.product_id_old == bs.product_id_new)\n",
    "   & (bs.culture_harvest_date_old == bs.culture_harvest_date_new)\n",
    "   & (bs.culture_start_date_old == bs.culture_start_date_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = biosamples_multi_match#.drop_duplicates('accession').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_multi_match.submitted_by_old.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bs_multi_match.groupby(bcols).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ANTIBODIES:\n",
    "product_id=A301-145A\n",
    "@type=AntibodyLot\n",
    "targets.gene_name: \"NCOR1\",\n",
    "antigen_description: \"Nuclear Receptor corepressor 1; N-CoR, TRAC1, KIAA1047, hN-CoR\",\n",
    "source.title: \"Bethyl Labs\",\n",
    "    \n",
    "https://www.encodeproject.org/search/?type=AntibodyLot&targets.gene_name=NCOR1&source.title=Bethyl+Labs&product_id=A301-145A&status%21=replaced\n",
    "    \n",
    "\n",
    "BIOSAMPLE\n",
    "biosample_type: \"immortalized cell line\",\n",
    "treatment: [ ]\n",
    "lab.name: \"gene-yeo\"\n",
    "culture_start_date: \"2015-06-12\",\n",
    "health_status: \"hepatocellular carcinoma\",\n",
    "product_id: \"HB-8065\",\n",
    "biosample_term_name: \"HepG2\",\n",
    "@type: \"Biosample\"\n",
    "donor.@id: \"/human-donors/ENCDO000AAC/\",\n",
    "summary: \"Homo sapiens HepG2 immortalized cell line\",\n",
    "life_stage: \"child\",\n",
    "source.title: \"ATCC\",\n",
    "biosample_term_name: \"HepG2\",\n",
    "    \n",
    "https://www.encodeproject.org/search/\n",
    "?type=Biosample&product_id=HB-8065\n",
    "&health_status=hepatocellular+carcinoma\n",
    "&culture_start_date=2015-06-12&status%21=replaced\n",
    "\n",
    "\n",
    "FILE\n",
    "quality_metrics.assay_term_name: \"ChIP-seq\",\n",
    "file_type: \"bam\",\n",
    "assembly: \"hg19\",\n",
    "lab.name: \"encode-processing-pipeline\",\n",
    "output_category: \"alignment\",\n",
    "analysis_step_version.analysis_step.name: \"bwa-raw-alignment-step-v-1\",\n",
    "biological_replicates: 1\n",
    "technical_replicates: [\n",
    "\"1_1\"\n",
    "        \n",
    "https://www.encodeproject.org/search/?type=File&file_format=bam\n",
    "&output_type=alignments&quality_metrics.assay_term_name=ChIP-seq\n",
    "&dataset=%2Fexperiments%2FENCSR021JFW%2F&assembly=hg19\n",
    "        \n",
    "LIBRARY\n",
    "nucleic_acid_term_name: \"DNA\",\n",
    "library_size_selection_method: \"SPRI beads\",\n",
    "strand_specificity: false,\n",
    "fragmentation_method: \"shearing (Covaris S2)\",\n",
    "aliases: \"tim-reddy:hic_dex.t0_brep1_lib\"\n",
    "lab: \"/labs/tim-reddy/\",\n",
    "crosslinking_method: \"formaldehyde\",\n",
    "biosample.summary: \"Homo sapiens A549 immortalized cell line\"\n",
    "biosample.biosample_term_name: \"A549\"\n",
    "        \n",
    "https://www.encodeproject.org/search/?type=Library\n",
    "&lab=%2Flabs%2Fthomas-gingeras%2F\n",
    "&nucleic_acid_term_name=polyadenylated+mRNA\n",
    "&strand_specificity=true&depleted_in_term_name=rRNA\n",
    "&biosample.biosample_term_name=NCI-H460\n",
    "&biosample.%40id=%2Fbiosamples%2FENCBS814QPR%2F&status%21=replaced\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
